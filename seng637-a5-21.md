**SENG 637- Dependability and Reliability of Software Systems\***

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:      | 21  |
| -------------- | --- |
| Student Names: |     |
| Carrie         |     |
| Jon            |     |
| Paolo          |     |
| Israel         |     |
| Henry          |     |

# Introduction

The goal of this lab is to explore the ways to assess failure data including reliability growth testing (part 1) and assessment using a reliability demonstration chart (part 2)

For part 1 this involves using a growth assessment tool. For this lab, the tool selected was C-SFRAT as for its user-friendly interface and easy setup. With this tool, the given test data will be imported to assess various models, and to analyze failure rate, MTTF and reliability metrics.

In part 2, a reliability demonstration chart tool will be used to determine if the target failure or MTTF criteria is met. The tool will give insight to the reliability trend of the SUT.

These two methods will be discussed and compared for their effectiveness as reliability assessment tools.

#

# Assessment Using Reliability Growth Testing

When testing the full dataset, each model/hazard method in C-SFRAT was applied to see which would provide the best fit to the data. This included the following:

- IFR Salvia & Bollinger, IFR generalized Salvia & Bollinger
- S Distribution
- Discrete Weibull (Order 2), Discrete Weibull (Type III)
- Geometric
- Negative Binomial (Order 2)
- Truncated Logistic

These Each model was also tested with varying covariates between E, F and C. This allows the variable to be predictors in the model to adjust the estimates of the primary effect. 57 combination were tested and the dataset was exported to a csv [model_results_group21](./model_results_group21.csv).

From the dataset, the ranking of the best 2 models were determined by considering the metric weights giving slightly more weight to the BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion) metrics. This is due to the nature of these methods that penalize overfitting, preventing unnecessary complexity. They are a metric that gives insight into the goodness of fit although have slight differences in their methodology. A lower value for either metric signifies a better fit. Other metrics were considered as well, such as Error Sum of Squares (SSE) to ensure these models had good performance across the various metrics.

In the dataset for all models, the top 2 both had the lowest scores for BIC and AIC and was determined by considering the critic method, which can rank based on the weighted combination of the metrics. The top 2 models can be seen below. These were 1. DW3 model with F covariate. and 2. Geometric with F covariate.

![top2table](/screenshots/top_2_table.png?raw=true)

The plot of time to failures plot for the top 2 models against the imported failure data can be seen below.

![top2plot](/screenshots/top_2_plot.png?raw=true)

When considering the range analysis of the dataset, subsets were tested sequentially to assess at which point, the model could continue to make relatively accurate predictions on the test data. It was observed that after including some points after the slope increase (around interval 19-22) provided enough information to the model to continue predicting accurately. The good fit that allowed to test against adequate testing points seemed to be using interval 0-22, which was about 70% of the training data. The remaining 20% (interval 23-31) was used to see if the model could continue predicting well.

The below image shows the top 2 models predicting the remaining 9 intervals of the testing data (interval 23-31) where the GM model is in orange and DW3 in blue. The best results came from adding more effort per interval to the F covariate.

![top2plot](/screenshots/top_2_predict.png?raw=true)

The intensity plot for the sub-interval can be seen below. The sofware was unable to show the predicted intensity for the remaining points on the plot. Those were manually calculated using the data from the table rather than the plot.

![top2plot](/screenshots/top_2_intensity.png?raw=true)

The failure rate was then determined using the full data to compare against the failure rate of the predictions from the DW3 and GM models.

| Dataset       | Failure Rate (F/interval) | MTTF (intervals) |
| ------------- | ------------------------- | ---------------- |
| original data | 92/31 = 2.96              | 1/2.96 = 0.337   |
| DW3 Predict   | 88/31 = 2.83              | 1/2.83 = 0.353   |
| GM Predict    | 92/21 = 2.96              | 1/2.96 = 0.337   |

The resulting failure rate from the original data and the predictions of the top 2 models are relatively close across the whole interval, suggesting the models were a good fit and continued to predict well across the remaining intervals.

### Decision making for target failure rate

Given the results, a company may base future performance against a set failure rate based on historical performance, similar to this lab. They can then use models to predict if the failure rate may become unacceptable in the future.

In this lab, if the failure data was an accurate representation of the company's operational conditions, a failure rate of 3 may be a good target value to maintain.

### Advantages and disadvantages of reliability growth testing

Some advantages include the predicting ability, allowing organizations to make informed decisions or allocate resources differently if needed. It can also be advantageous to identify problematic areas such as surges in the systems reliability and failures.

The disadvantages include the difficulty to actually predict reliability. Results may be different and there are often many external variables that may alter and influcence the failures. Additionally, this analysis requires adequate collected data on the system's reliability. This is not always available, accurate or easy to obtain. This may be a hurdle for organizations to correctly implement.

# Assessment Using Reliability Demonstration Chart

## Data Reformatting

Since the given data describes failure count at individual time intervals, the data needs to be reformatted before plotting it on the RDC. Assuming that the failures are evenly spread throughout the time interval they take place in, the time period for each individual failure can be determined. Therefore the cumulative time for each failure can also be determined, giving us the "timestamp" at which each failure occured.

## Configuration

The given RDC Excel sheet seems to only be able to handle up to 16 data points, so the first 16 data points of the failure data was used. The default risk profile was used:

- Discrimination Ratio: 2.0
- Developer's Risk: 0.10
- User's Risk: 0.10

## Results

In order to find the minimum MTTF for the SUT to be considered acceptable, no points in the graph should be in the red "Reject" region. Using trial and error on the "Failure Intensity Objective" parameters, the **minimum FIO to achieve an acceptable SUT is 5 failures per time interval**. 

The minimum MTTF is simply the inverse of the minimum FIO: 1/5 = 0.2 intervals per failure.

Below is the plotted data using the minimum FIO.

![rdc_min_fio](/screenshots/rdc_min_fio.png)

Setting the MTTF to twice the min MTTF results in an MTTF of 0.4 intervals per failure, therefore a FIO of 2.5 failures per interval:

![rdc_min_fio_half](/screenshots/rdc_min_fio_half.png)

Setting the MTTF to half the minimum MTTF results in an MTTF of 0.1 intervals per failure, therefore a FIO of 10 failures per interval:

![rdc_min_fio_double](/screenshots/rdc_min_fio_double.png)

Since the MTTF and FIO are the inverse of each other, these results make sense. Doubling the MTTF results in halving the FIO, creating more strict regions in the RDC. Halving the MTTF results in doubling the FIO, resulting in a more lenient RDC.

The results of the RDC are not a good representation of the system. An assumption was made that the failures are evenly distributed over their time interval, which is likely not the case. The RDC itself is only limited to 16 data points as its input, which is less than 20% of the entire dataset.

### Advantages and Disadvantages

Some advantages of using a RDC include its versatility, and its time and cost efficiency for analyzing a system's reliability. It also provides a clear visual representation of reliability data that can be easier to interpret. 

A disadvantage of the RDC is that it cannot be used to calculate the exact quantitative value for the reliability or availability of the SUT. Another disadvantage is that the effectiveness and the accuracy of an RDC is dependent on the quality and completeness of the input data. 

# Comparison of Results

Due to the nature of each test, it is difficult to compare the results of the Reliability Growth Testing to the RDC. The goal of the Reliability Growth Testing is to predict the future reliability of the system, while the RDC focuses on current data to determine if the system should be rejected, accepted, or undergo more testing. 

Both models in the Reliability Growth Tests (the DW3 and GM models) predicted an MTTF of ~0.34 and failure rate of ~3, which is relatively high compared to the minimum acceptable MTTF of 0.2 and FIO of 5 found in the RDC. This can indicate that the Reliability Growth Tests predict that the SUT will meet the acceptance criteria, since it predicts a lower failure rate and higher MTTF. 

# Discussion on Similarity and Differences of the Two Techniques

The similiarities between the two techniques are that they both utilize historical data involving failures at certain intervals to achieve their goals.

The difference lies in that two techniques seem to be the inverse of each other. Reliability Growth Testing uses historical data to predict failure rates and MTTF, while RDC uses historical data and plots it against a given FIO to assess whether the SUT is acceptable or not. 

# How the team work/effort was divided and managed

The team split into two teams, where one team performed the Reliability Growth Testing and the other team completed the RDC. The teams came together at the end to discuss and compare results, and similarities/differences between the two techniques. 

# Difficulties encountered, challenges overcome, and lessons learned

The biggest challenge we faced in the RDC portion of the lab was during the data entry stage. Reformatting the input data to work on the RDC took some trial and error and some assumptions that might not be valid. The RDC itself doesn't seem to be the best fit for the SUT, due to the limit of 16 data points. We tried using the whole dataset and tinkering with the "behind the scenes" calculations to allow for more data points, but that proved to be difficult and we ended up settling with using the first 16 points.

# Comments/feedback on the lab itself

The lab as a whole served as a somewhat gentle introduction to Reliability Growth Testing and RDC. The RDC in this lab is not well suited to the given dataset, since the it can only accept up to 16 data points. A large portion of the time spent in the RDC part consisted of tinkering - whether it was trying to correctly format the data or fit all of it into the chart. 
